{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Keras-RNN.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfHxGTarkRVo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Sentiment analysis from movie reviews\n",
        "Import everythin we will need\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXfdLCgukRVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.datasets import imdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lfC43U5kRVy",
        "colab_type": "text"
      },
      "source": [
        "We pick the first 20,000 most popular interviews from the dataset to save on time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn2elpBkkRVz",
        "colab_type": "code",
        "colab": {},
        "outputId": "23ad1c4e-0296-4a2c-cabc-ce143c6f7730"
      },
      "source": [
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aTi9M1qkRV5",
        "colab_type": "text"
      },
      "source": [
        "We display the data keep in mind the number actually represent words like 1 may correspond with the word \"the\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiwgVkVMkRV7",
        "colab_type": "code",
        "colab": {},
        "outputId": "c109bbba-cadf-4b6e-ac74-f7b1dd2d97a7"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 19193,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 10311,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 12118,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShaHUrxvkRWB",
        "colab_type": "text"
      },
      "source": [
        "0 means a negative review and 1 means a positive review\n",
        "\n",
        "What do the labels look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLt9JTLQkRWB",
        "colab_type": "code",
        "colab": {},
        "outputId": "066ac1da-687f-4e47-ea05-0eaceafa8d87"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs2y-UovkRWE",
        "colab_type": "text"
      },
      "source": [
        "We limit the total number of words from each review at only the first 80 words once again to save on time. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKKETpYukRWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = sequence.pad_sequences(x_train, maxlen=80)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMgIonIPkRWH",
        "colab_type": "text"
      },
      "source": [
        "Now let's set up our neural network model! Considering how complicated a LSTM recurrent neural network is under the hood, it's really amazing how easy this is to do with Keras.\n",
        "\n",
        "We will start with an Embedding layer - this is just a step that converts the input data into dense vectors of fixed size that's better suited for a neural network. You generally see this in conjunction with index-based text data like we have here. The 20,000 indicates the vocabulary size (remember we said we only wanted the top 20,000 words) and 128 is the output dimension of 128 units.\n",
        "\n",
        "Next we just have to set up a LSTM layer for the RNN itself. It's that easy. We specify 128 to match the output size of the Embedding layer, and dropout terms to avoid overfitting, which RNN's are particularly prone to.\n",
        "\n",
        "Finally we just need to boil it down to a single neuron with a sigmoid activation function to choose our binay sentiment classification of 0 or 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vs_F9ihkRWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(20000, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe35D8ozkRWK",
        "colab_type": "text"
      },
      "source": [
        "As this is a binary classification problem, we'll use the binary_crossentropy loss function we choose the ADAM optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYuArm3wkRWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OGke2mIkRWM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_SRlgTbkRWM",
        "colab_type": "text"
      },
      "source": [
        "We set a batch size of 32 reviews and 15 epochs(the number of times the model is trained)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr7YXlyckRWN",
        "colab_type": "code",
        "colab": {},
        "outputId": "795ff301-5943-42f4-82df-d836c8ce9197"
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=15,\n",
        "          verbose=2,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/15\n",
            "25000/25000 - 83s - loss: 0.4605 - accuracy: 0.7798 - val_loss: 0.3795 - val_accuracy: 0.8349\n",
            "Epoch 2/15\n",
            "25000/25000 - 80s - loss: 0.3014 - accuracy: 0.8780 - val_loss: 0.4075 - val_accuracy: 0.8237\n",
            "Epoch 3/15\n",
            "25000/25000 - 81s - loss: 0.2124 - accuracy: 0.9182 - val_loss: 0.4114 - val_accuracy: 0.8238\n",
            "Epoch 4/15\n",
            "25000/25000 - 81s - loss: 0.1520 - accuracy: 0.9445 - val_loss: 0.4581 - val_accuracy: 0.8271\n",
            "Epoch 5/15\n",
            "25000/25000 - 79s - loss: 0.1097 - accuracy: 0.9603 - val_loss: 0.5656 - val_accuracy: 0.8252\n",
            "Epoch 6/15\n",
            "25000/25000 - 78s - loss: 0.0772 - accuracy: 0.9724 - val_loss: 0.6683 - val_accuracy: 0.8210\n",
            "Epoch 7/15\n",
            "25000/25000 - 79s - loss: 0.0622 - accuracy: 0.9790 - val_loss: 0.7128 - val_accuracy: 0.8209\n",
            "Epoch 8/15\n",
            "25000/25000 - 81s - loss: 0.0366 - accuracy: 0.9877 - val_loss: 0.9249 - val_accuracy: 0.7985\n",
            "Epoch 9/15\n",
            "25000/25000 - 79s - loss: 0.0291 - accuracy: 0.9912 - val_loss: 0.9049 - val_accuracy: 0.8130\n",
            "Epoch 10/15\n",
            "25000/25000 - 85s - loss: 0.0255 - accuracy: 0.9914 - val_loss: 0.8627 - val_accuracy: 0.8105\n",
            "Epoch 11/15\n",
            "25000/25000 - 86s - loss: 0.0222 - accuracy: 0.9930 - val_loss: 0.9969 - val_accuracy: 0.8167\n",
            "Epoch 12/15\n",
            "25000/25000 - 82s - loss: 0.0178 - accuracy: 0.9940 - val_loss: 1.0226 - val_accuracy: 0.8143\n",
            "Epoch 13/15\n",
            "25000/25000 - 88s - loss: 0.0109 - accuracy: 0.9970 - val_loss: 1.1139 - val_accuracy: 0.8104\n",
            "Epoch 14/15\n",
            "25000/25000 - 89s - loss: 0.0094 - accuracy: 0.9971 - val_loss: 1.2101 - val_accuracy: 0.8125\n",
            "Epoch 15/15\n",
            "25000/25000 - 86s - loss: 0.0066 - accuracy: 0.9980 - val_loss: 1.1447 - val_accuracy: 0.8053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x1abc5012128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvtM8WQdkRWP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QJOvaV8kRWP",
        "colab_type": "code",
        "colab": {},
        "outputId": "add5ec51-7a55-4d39-8979-db7bfd0c3bc4"
      },
      "source": [
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=32,\n",
        "                            verbose=2)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/1 - 14s - loss: 1.3717 - accuracy: 0.8053\n",
            "Test score: 1.144708181578219\n",
            "Test accuracy: 0.80528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoOv8z6ukRWS",
        "colab_type": "text"
      },
      "source": [
        "The overall accuracy is low at 80.5% however we only looked at the first 80 words of the review had this been done with all the words the accuracy would be much higher but the total time to run the model on the test data would be longer as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oupZX5KQkRWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}